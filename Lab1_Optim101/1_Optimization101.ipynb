{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)  -- 1st year</h3></center>\n",
    "<hr>\n",
    "<center><h1>Optimization</h1></center>\n",
    "<center><h2>Lab1: Optimization 101 (3h)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of an optimization program\n",
    "\n",
    "An optimization program can be practically divided into three parts:\n",
    "* the *run* environment, in which you test, run your program, and display results.\n",
    "* the *problem* part, which contains the function oracles, problem constraints, etc.\n",
    "* the *algorithmic* part, where the algorithms are coded.\n",
    "\n",
    "The main interest of such division is that these parts are interchangeable, meaning that, for instance, the algorithms of the third part can be used of a variety of problems. That is why such a decomposition is widely used.\n",
    "\n",
    "In the present lab, you will use this division:\n",
    "* `1_Optimization101.ipynb` will be the *run* environment\n",
    "* `problem1.ipynb` .. `problem5.ipynb` will be the considered *problems* for this lab (see <a href=\"#pbs\">Problems</a>)\n",
    "* `algoGradient.ipynb` will contain the gradient *algorithms* studied in this lab\n",
    "\n",
    "---\n",
    "\n",
    "The following script will allow you to import *notebooks* as if you imported *python files* and will have to be executed at each time you launch Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import start\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Gradient algorithms on a simple function\n",
    "\n",
    "We begin by investigating <a href=\"#pb3\">Problem 1</a>\n",
    "\n",
    "> Observe the 3D and level plots of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import problem1 as pb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "custom_3dplot( pb1.f, pb1.x1_min,pb1.x1_max,pb1.x2_min,pb1.x2_max,pb1.nb_points, pb1.vmin, pb1.vmax )\n",
    "level_plot( pb1.f, pb1.x1_min,pb1.x1_max,pb1.x2_min,pb1.x2_max,pb1.nb_points, pb1.levels , pb1.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a. simple gradient algorithm\n",
    "\n",
    "> Examine the functions and variables defined in `problem1.ipynb` <br/>\n",
    "> Complete the function `gradient_algorithm` as asked in `algoGradient.ipynb [Sec. 1a]` <br/>\n",
    "> Examine and run the cells below. Notably change the step size `step` and observe the different behaviors of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "\n",
    "import problem1 as pb1\n",
    "reload(pb1) \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 20                       # Max number of iterations\n",
    "x0      = np.array( (0.0,0.0 ) )   # Initial point\n",
    "step    = 1.0/pb1.L                # Stepsize \n",
    "\n",
    "\n",
    "##### Gradient algorithm\n",
    "\n",
    "\n",
    "x,x_tab = gradient_algorithm(pb1.f , pb1.f_grad , x0 , step , PREC , ITE_MAX )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting\n",
    "\n",
    "The following cell plots the iterates over the level sets of the minimized function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "level_points_plot( pb1.f , x_tab , pb1.x1_min,pb1.x1_max,pb1.x2_min,pb1.x2_max,pb1.nb_points, pb1.levels , pb1.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.b. Newton algorithm\n",
    "\n",
    "> Examine the functions and variables defined in `problem1.ipynb` <br/>\n",
    "> Complete the function `newton_algorithm` as asked in `algoGradient.ipynb [Sec. 2]` <br/>\n",
    "> Examine and run the cells below. In how many iterations does the algorithm converge? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "\n",
    "import problem1 as pb1\n",
    "reload(pb1) \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 20                       # Max number of iterations\n",
    "x0      = np.array( (0.0,0.0 ) )   # Initial point\n",
    "\n",
    "##### Newton algorithm\n",
    "xn,xn_tab = newton_algorithm(pb1.f , pb1.f_grad_hessian , x0  , PREC , ITE_MAX )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "level_points_plot( pb1.f , xn_tab , pb1.x1_min,pb1.x1_max,pb1.x2_min,pb1.x2_max,pb1.nb_points, pb1.levels , pb1.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Comparing gradient algorithm and Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "##### comparison\n",
    "level_2points_plot( pb1.f , x_tab , xn_tab ,  pb1.x1_min, pb1.x1_max, pb1.x2_min, pb1.x2_max, pb1.nb_points,  pb1.levels ,  pb1.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. More involved functions\n",
    "\n",
    "Now, we investigate other functions and examine the behavior of gradient algorithms in these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a  Shaper functions\n",
    "\n",
    "<a href=\"#pb3\">Problem 2</a> features a sharper function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import problem2 as pb2\n",
    "\n",
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "custom_3dplot( pb2.f, pb2.x1_min,pb2.x1_max,pb2.x2_min,pb2.x2_max,pb2.nb_points, pb2.vmin, pb2.vmax )\n",
    "level_plot( pb2.f, pb2.x1_min,pb2.x1_max,pb2.x2_min,pb2.x2_max,pb2.nb_points, pb2.levels , pb2.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fill the needed 1st and 2nd order oracles in `problem2.ipynb`.<br/>\n",
    "> Run and compare constant stepsize gradient and Newton algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "import numpy as np\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 20                       # Max number of iterations\n",
    "x0      = np.array( (0.0,0.0 ) )   # Initial point\n",
    "\n",
    "##### Gradient algorithm\n",
    "step    = 1.0/pb2.L                # Stepsize \n",
    "x,x_tab = gradient_algorithm(pb2.f , pb2.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### Newton algorithm\n",
    "xn,xn_tab = newton_algorithm(pb2.f , pb2.f_grad_hessian , x0  , PREC , ITE_MAX )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Comparing gradient algorithm and Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "##### comparison\n",
    "level_points_plot( pb2.f , x_tab , pb2.x1_min,pb2.x1_max,pb2.x2_min,pb2.x2_max,pb2.nb_points, pb2.levels , pb2.title )\n",
    "#level_points_plot( pb2.f , xn_tab , pb2.x1_min,pb2.x1_max,pb2.x2_min,pb2.x2_max,pb2.nb_points, pb2.levels , pb2.title )\n",
    "#level_2points_plot( pb2.f , x_tab , xn_tab ,  pb2.x1_min, pb2.x1_max, pb2.x2_min, pb2.x2_max, pb2.nb_points,  pb2.levels ,  pb2.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Non-convex functions\n",
    "\n",
    "<a href=\"#pb3\">Problem 3</a> features classical Rosenbrock non-convex function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import problem3 as pb3\n",
    "\n",
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "custom_3dplot( pb3.f, pb3.x1_min,pb3.x1_max,pb3.x2_min,pb3.x2_max,pb3.nb_points, pb3.vmin, pb3.vmax )\n",
    "level_plot( pb3.f, pb3.x1_min,pb3.x1_max,pb3.x2_min,pb3.x2_max,pb3.nb_points, pb3.levels , pb3.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fill the needed 1st and 2nd order oracles in `problem3.ipynb`.<br/>\n",
    "> Run and compare constant stepsize gradient and Newton algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "import numpy as np\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 20                       # Max number of iterations\n",
    "x0      = np.array( (0.0,0.0 ) )   # Initial point\n",
    "\n",
    "##### Gradient algorithm\n",
    "step    =  0.1                     # Stepsize \n",
    "x,x_tab = gradient_algorithm(pb3.f , pb3.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### Newton algorithm\n",
    "xn,xn_tab = newton_algorithm(pb3.f , pb3.f_grad_hessian , x0  , PREC , ITE_MAX )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Comparing gradient algorithm and Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "##### comparison\n",
    "level_points_plot( pb3.f , x_tab , pb3.x1_min,pb3.x1_max,pb3.x2_min,pb3.x2_max,pb3.nb_points, pb3.levels , pb3.title )\n",
    "#level_points_plot( pb3.f , xn_tab , pb3.x1_min,pb3.x1_max,pb3.x2_min,pb3.x2_max,pb3.nb_points, pb3.levels , pb3.title )\n",
    "#level_2points_plot( pb3.f , x_tab , xn_tab ,  pb3.x1_min, pb3.x1_max, pb3.x2_min, pb3.x2_max, pb3.nb_points,  pb3.levels ,  pb3.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> try and correct the gradient algorithm behavior by creating an adaptative stepsize algorithm in `algoGradient.ipynb [Sec. 1b]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Non-convex functions with mutliple minimizers\n",
    "\n",
    "<a href=\"#pb4\">Problem 4</a> features a smooth non-convex function with two minimizers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import problem4 as pb4\n",
    "\n",
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "custom_3dplot( pb4.f, pb4.x1_min,pb4.x1_max,pb4.x2_min,pb4.x2_max,pb4.nb_points, pb4.vmin, pb4.vmax )\n",
    "level_plot( pb4.f, pb4.x1_min,pb4.x1_max,pb4.x2_min,pb4.x2_max,pb4.nb_points, pb4.levels , pb4.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fill the needed 1st order oracles in `problem4.ipynb`. Can you compute a second order oracle?<br/>\n",
    "> Run the constant and adaptive stepsize gradient algorithms. Change the initialization and observe the final point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "import numpy as np\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 100                       # Max number of iterations\n",
    "x0      = np.array( (1.5,0.55 ) )   # Initial point\n",
    "\n",
    "##### Gradient algorithm\n",
    "step    =  0.1                     # Stepsize \n",
    "x,x_tab = gradient_algorithm(pb4.f , pb4.f_grad , x0 , step , PREC , ITE_MAX )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Comparing gradient algorithm and Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "##### comparison\n",
    "level_points_plot( pb4.f , x_tab , pb4.x1_min,pb4.x1_max,pb4.x2_min,pb4.x2_max,pb4.nb_points, pb4.levels , pb4.title )\n",
    "#level_points_plot( pb4.f , xn_tab , pb4.x1_min,pb4.x1_max,pb4.x2_min,pb4.x2_max,pb4.nb_points, pb4.levels , pb4.title )\n",
    "#level_2points_plot( pb4.f , x_tab , xn_tab ,  pb4.x1_min, pb4.x1_max, pb4.x2_min, pb4.x2_max, pb4.nb_points,  pb4.levels ,  pb4.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What would you do if you would like to find *all* minimizers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d Polyhedral functions\n",
    "\n",
    "<a href=\"#pb5\">Problem 5</a> features a convex polydehral function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import problem5 as pb5\n",
    "\n",
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "custom_3dplot( pb5.f, pb5.x1_min,pb5.x1_max,pb5.x2_min,pb5.x2_max,pb5.nb_points, pb5.vmin, pb5.vmax )\n",
    "level_plot( pb5.f, pb5.x1_min,pb5.x1_max,pb5.x2_min,pb5.x2_max,pb5.nb_points, pb5.levels , pb5.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fill the needed 1st order oracles in `problem5.ipynb`. What is different from before?<br/>\n",
    "> Run the constant and adaptive stepsize gradient algorithms. Change the initialization and observe the final point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "import numpy as np\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 100                       # Max number of iterations\n",
    "x0      = np.array( (1.5,0.55 ) )   # Initial point\n",
    "\n",
    "##### Gradient algorithm\n",
    "step    =  0.1                     # Stepsize \n",
    "x,x_tab = gradient_algorithm(pb5.f , pb5.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "xa,xa_tab = gradient_adaptive_algorithm(pb5.f , pb5.f_grad , x0 , step , PREC , ITE_MAX )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Comparing gradient algorithm and adaptive stepsize method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "##### comparison\n",
    "#level_points_plot( pb5.f , x_tab , pb5.x1_min,pb5.x1_max,pb5.x2_min,pb5.x2_max,pb5.nb_points, pb5.levels , pb5.title )\n",
    "#level_points_plot( pb5.f , xa_tab , pb5.x1_min,pb5.x1_max,pb5.x2_min,pb5.x2_max,pb5.nb_points, pb5.levels , pb5.title )\n",
    "level_2points_plot( pb5.f , x_tab , xa_tab ,  pb5.x1_min, pb5.x1_max, pb5.x2_min, pb5.x2_max, pb5.nb_points,  pb5.levels ,  pb5.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What would you do to minimize polyhedral functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: <a id=\"pbs\">Problems</a>\n",
    "\n",
    "The problems we consider in this first lab are minimizations of unconstrained continous functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **1.** <a id=\"pb3\">`problem1`</a> features a simple quadratic function\n",
    "$$\\begin{array}{rrcll}\n",
    "f: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & 4 (x_1-3)^2 + 2(x_2-1)^2\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/1.png\" width=\"50%\"></center>\n",
    "\n",
    "\n",
    "> **2.** <a id=\"pb3\">`problem2`</a> features a more involved but very smooth function\n",
    "$$\\begin{array}{rrcll}\n",
    "g: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & \\log( 1 + \\exp(4 (x_1-3)^2 ) + \\exp( 2(x_2-1)^2 ) ) - \\log(3)\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/2.png\" width=\"50%\"></center>\n",
    "\n",
    "\n",
    "> **3.** <a id=\"pb3\">`problem3`</a> features Rosenbrock's smooth but non-convex function\n",
    "$$\\begin{array}{rrcll}\n",
    "r: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  &  (1-x_1)^2 + 100(x_2-x_1^2)^2\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/3.png\" width=\"50%\"></center>\n",
    "\n",
    "\n",
    "> **4.** <a id=\"pb4\">`problem4`</a> features a smooth function with two distinct minimizers\n",
    "$$\\begin{array}{rrcll}\n",
    "t: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & (0.6 x_1 + 0.2 x_2)^2 \\left((0.6 x_1 + 0.2 x_2)^2 - 4 (0.6 x_1 + 0.2 x_2)+4\\right) + (-0.2 x_1 + 0.6 x_2)^2\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/4.png\" width=\"50%\"></center>\n",
    "\n",
    "\n",
    "> **5.** <a id=\"pb5\">`problem5`</a> features a polyhedral function\n",
    "$$\\begin{array}{rrcll}\n",
    "p: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  &  \\left| x_1-3 \\right|  + 2\\left| x_2-1\\right| .\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/5.png\" width=\"50%\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
