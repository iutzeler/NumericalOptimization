{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)  - 1st year</h3></center>\n",
    "<hr>\n",
    "<center><h1>Optimization</h1></center>\n",
    "<center><h2>Lab on the gradient algorithm </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "The following script will allow you to import *notebooks* as if you imported *python files* and will have to be executed at each time you launch Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import start\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Line-search\n",
    "\n",
    "In the previous Lab, we saw that it can be difficult to choose a satisfying stepsize.\n",
    "\n",
    "An option to choose a satisfying stepsize $\\gamma$ is to test different stepsizes by calling succesively the function oracles. Wolfe's line-search is implemented in `Scipy`'s <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.line_search.html\">`scipy.optimize.line_search`</a>. \n",
    "\n",
    "\n",
    "**Wolfe's line-search.** Let $x$ be the current point, $d$ a descent direction, and $q(\\gamma)=f(x+\\gamma d)$.Wolfe's line-search consists in deciding that \n",
    "* $\\gamma$ is *satisfying* if $q(\\gamma)\\leq q(0)+m_1 \\gamma q'(0)$ and $q'(\\gamma)\\geq m_2 q'(0)$;\n",
    "* $\\gamma$ is *too big* if $q(\\gamma) > q(0)+m_1 \\gamma q'(0)$;\n",
    "* $\\gamma$ is *too small* if $q(\\gamma)\\leq q(0)+m_1 \\gamma q'(0)$ and $q'(\\gamma)<m_2 q'(0)$;\n",
    "\n",
    "for two constants $0<m_1<m_2<1$, for instance: $m1 = 0.0001, m2 = 0.9$. The method consists in starting from a search interval $[\\gamma_1,\\gamma_2]$ and testing $\\gamma' = (\\gamma_1 + \\gamma_2)/2$. If $\\gamma'$ is too big, then  $[\\gamma_1,\\gamma_2] \\leftarrow [\\gamma_1,\\gamma']$; if it is too small $[\\gamma_1,\\gamma_2] \\leftarrow [\\gamma',\\gamma_2]$; if it is acceptable, choose $\\gamma'$. This method is provably convergent in a finite number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let us consider the opposite of the gradient  as a descent direction. The objective of this section is to implemetn and observe the behavior of gradient algorithms with Wolfe's line-search.  \n",
    "\n",
    "\n",
    "> Complete the function `gradient_Wolfe` in `algoGradient.ipynb [Sec. 1c]`. <br/>\n",
    "> Compare the convergence of this gradient with other gradient methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "### 1a. Comparing constant stepsize gradient algorithm and Wolfe search on Problem 1\n",
    "\n",
    "> Print the stepsizes chosen by line search and compare with theoretical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "\n",
    "import numpy as np\n",
    "import problem1 as pb1\n",
    "\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 20                       # Max number of iterations\n",
    "x0      = np.array( (0.0,0.0 ) )   # Initial point\n",
    "step    = 0.1\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = gradient_algorithm(pb1.f , pb1.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### Wolfe line-search algorithm\n",
    "xW,xW_tab = gradient_Wolfe(pb1.f , pb1.f_grad , x0 , PREC , ITE_MAX )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "##### comparison\n",
    "level_2points_plot( pb1.f , x_tab , xW_tab ,  pb1.x1_min, pb1.x1_max, pb1.x2_min, pb1.x2_max, pb1.nb_points,  pb1.levels ,  pb1.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "### 1b. Comparing constant stepsize gradient algorithm and Wolfe search on Problem 2\n",
    "\n",
    "> Try different starting points and observe the results of line search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "\n",
    "import numpy as np\n",
    "import problem2 as pb2\n",
    "\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 20                       # Max number of iterations\n",
    "x0      = np.array( (1.5,1.5 ) )   # Initial point\n",
    "step    = 0.1\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = gradient_algorithm(pb2.f , pb2.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### Wolfe line-search algorithm\n",
    "xW,xW_tab = gradient_Wolfe(pb2.f , pb2.f_grad , x0 , PREC , ITE_MAX )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "##### comparison\n",
    "level_2points_plot( pb2.f , x_tab , xW_tab ,  pb2.x1_min, pb2.x1_max, pb2.x2_min, pb2.x2_max, pb2.nb_points,  pb2.levels ,  pb2.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1c. Comparing constant stepsize gradient algorithm and Wolfe search on Problem 3\n",
    "\n",
    "> Compare the convergence of the gradient with and without line search. Keeping in mind that Newton method takes around $30$ iterations to converge, what is the biggest problem for minimizing such function, the stepsize or the descent direction?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "\n",
    "import numpy as np\n",
    "import problem3 as pb3\n",
    "\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.0001                     # Sought precision\n",
    "ITE_MAX = 10000                       # Max number of iterations\n",
    "x0      = np.array( (-1.0,1.2 ) )   # Initial point\n",
    "step    = 0.001\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = gradient_algorithm(pb3.f , pb3.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### Wolfe line-search algorithm\n",
    "xW,xW_tab = gradient_Wolfe(pb3.f , pb3.f_grad , x0 , PREC , ITE_MAX )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "##### comparison\n",
    "level_2points_plot( pb3.f , x_tab , xW_tab ,  pb3.x1_min, pb3.x1_max, pb3.x2_min, pb3.x2_max, pb3.nb_points,  pb3.levels ,  pb3.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1d. Comparing constant stepsize gradient algorithm and Wolfe search on Problem 5\n",
    "\n",
    "> Try different starting points $(0,0)$ , $(0,1)$, $(1,0)$, $(0.2,0.4)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "\n",
    "import numpy as np\n",
    "import problem5 as pb5\n",
    "\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.001                     # Sought precision\n",
    "ITE_MAX = 100                       # Max number of iterations\n",
    "x0      = np.array( (0.,0. ) )   # Initial point\n",
    "step    = 0.1\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = gradient_algorithm(pb5.f , pb5.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### Wolfe line-search algorithm\n",
    "xW,xW_tab = gradient_Wolfe(pb5.f , pb5.f_grad , x0 , PREC , ITE_MAX )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "##### comparison\n",
    "level_2points_plot( pb5.f , x_tab , xW_tab ,  pb5.x1_min, pb5.x1_max, pb5.x2_min, pb5.x2_max, pb5.nb_points,  pb5.levels ,  pb5.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Quasi Newton\n",
    "\n",
    "Now that we have a proper way of choosing a good stepsize, we see that the opposite of the gradient is not always a good descent direction. We saw in the previous Lab that Newton method was sometimes computationally expensive. In this section, we investigate a method to choose descent directions based on the approximation of the inverse Hessian.\n",
    "\n",
    "For a differentiable function $f$, Quasi-Newton methods iteratively construct an approximation $W_k$ of the inverse of the Hessian then use descent direction $-W_k\\nabla f(x_k)$.\n",
    "\n",
    "**BFGS.** (Broyden-Fletcher-Goldfarb-Shanno, 1970) The popular BFGS algorithm consist in performing the following iteration\n",
    "$$ x_{k+1}=x_k - \\gamma_k W_k \\nabla f(x_k)$$\n",
    "where $\\gamma_k$ is given by Wolfe's line-search and positive definite matrix $W_k$ is computed as\n",
    "$$ W_{k+1}=W_k - \\frac{s_k y_k^T W_k+W_k y_k s_k^T}{y_k^T s_k} +\\left[1+\\frac{y_k^T W_k y_k}{y_k^T s_k}\\right]\\frac{s_k s_k^T}{y_k^T s_k} $$\n",
    "with $s_k=x_{k+1}-x_{k}$ and $y_k=\\nabla f(x_{k+1}) - \\nabla f(x_{k})$.\n",
    "\n",
    "The general scheme is then:\n",
    "* from initial point $x_0$, and initial positive definite matrix $W_0$;\n",
    "* from gradient $\\nabla f(x_k)$, compute direction $d_k=-W_k \\nabla f(x_k)$;\n",
    "* compute stepsize $\\gamma_k$ by Wolfe's line-search;\n",
    "* from new point $x_{k+1}$, call the function oracle and compute $W_{k+1}$.\n",
    "\n",
    "> Implement BFGS method in `algoGradient.ipynb [Section 3]`.\n",
    "\n",
    "*Hint: Use fonction `np.outer(a,b)` to compute $ab^T$.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Comparing constant stepsize gradient algorithm and Wolfe search on Problem 3\n",
    "\n",
    "> Compare the convergence of the gradient with line search and BFGS; then Newton vs BFGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "\n",
    "import numpy as np\n",
    "import problem3 as pb3\n",
    "\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 1e-4                    # Sought precision\n",
    "ITE_MAX = 10000                       # Max number of iterations\n",
    "x0      = np.array( (-1.0,1.2 ) )   # Initial point\n",
    "\n",
    "##### Wolfe line-search algorithm\n",
    "xW,xW_tab = gradient_Wolfe(pb3.f , pb3.f_grad , x0 , PREC , ITE_MAX )\n",
    "\n",
    "##### Newton algorithm\n",
    "xN,xN_tab = newton_algorithm(pb3.f , pb3.f_grad_hessian , x0 , PREC , ITE_MAX )\n",
    "\n",
    "##### BFGS algorithm\n",
    "xB,xB_tab = bfgs(pb3.f , pb3.f_grad , x0 , PREC , ITE_MAX )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plots\n",
    "\n",
    "* Gradient with line search vs BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "##### comparison\n",
    "level_2points_plot( pb3.f , xW_tab , xB_tab ,  pb3.x1_min, pb3.x1_max, pb3.x2_min, pb3.x2_max, pb3.nb_points,  pb3.levels ,  pb3.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Newton vs BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "##### comparison\n",
    "level_2points_plot( pb3.f , xN_tab , xB_tab ,  pb3.x1_min, pb3.x1_max, pb3.x2_min, pb3.x2_max, pb3.nb_points,  pb3.levels ,  pb3.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: <a id=\"pbs\">Problems</a>\n",
    "\n",
    "The problems we consider in this first lab are minimizations of unconstrained continous functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **1.** <a id=\"pb3\">`problem1`</a> features a simple quadratic function\n",
    "$$\\begin{array}{rrcll}\n",
    "f: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & 4 (x_1-3)^2 + 2(x_2-1)^2\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/1.png\" width=\"50%\"></center>\n",
    "\n",
    "\n",
    "> **2.** <a id=\"pb3\">`problem2`</a> features a more involved but very smooth function\n",
    "$$\\begin{array}{rrcll}\n",
    "g: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & \\log( 1 + \\exp(4 (x_1-3)^2 ) + \\exp( 2(x_2-1)^2 ) ) - \\log(3)\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/2.png\" width=\"50%\"></center>\n",
    "\n",
    "\n",
    "> **3.** <a id=\"pb3\">`problem3`</a> features Rosenbrock's smooth but non-convex function\n",
    "$$\\begin{array}{rrcll}\n",
    "r: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  &  (1-x_1)^2 + 100(x_2-x_1^2)^2\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/3.png\" width=\"50%\"></center>\n",
    "\n",
    "\n",
    "> **4.** <a id=\"pb4\">`problem4`</a> features a smooth function with two distinct minimizers\n",
    "$$\\begin{array}{rrcll}\n",
    "t: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & (0.6 x_1 + 0.2 x_2)^2 \\left((0.6 x_1 + 0.2 x_2)^2 - 4 (0.6 x_1 + 0.2 x_2)+4\\right) + (-0.2 x_1 + 0.6 x_2)^2\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/4.png\" width=\"50%\"></center>\n",
    "\n",
    "\n",
    "> **5.** <a id=\"pb5\">`problem5`</a> features a polyhedral function\n",
    "$$\\begin{array}{rrcll}\n",
    "p: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  &  \\left| x_1-3 \\right|  + 2\\left| x_2-1\\right| .\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/5.png\" width=\"50%\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
