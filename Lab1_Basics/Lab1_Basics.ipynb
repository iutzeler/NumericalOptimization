{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)  - 1st year</h3></center>\n",
    "<hr>\n",
    "<center><h1>Numerical Optimization</h1></center>\n",
    "<center><h2>Lab 1: Basics </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of an optimization program\n",
    "\n",
    "An optimization program can be practically divided into three parts:\n",
    "* the *run* environment, in which you test, run your program, and display results.\n",
    "* the *problem* part, which contains the function oracles, problem constraints, etc.\n",
    "* the *algorithmic* part, where the algorithms are coded.\n",
    "\n",
    "The main interest of such division is that these parts are interchangeable, meaning that, for instance, the algorithms of the third part can be used of a variety of problems. That is why such a decomposition is widely used.\n",
    "\n",
    "In the present lab, you will use this division:\n",
    "* `1_Optimization101.ipynb` will be the *run* environment\n",
    "* `problem1.py` .. `problem5.py` will be the considered *problems* for this lab (see <a href=\"#pbs\">Problems</a>)\n",
    "* `algorithms.py` will contain the gradient *algorithms* studied in this lab\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 # To automatically reload external modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Gradient algorithms on a simple function\n",
    "\n",
    "We begin by investigating <a href=\"#pb3\">Problem 1</a>\n",
    "\n",
    "> Observe the 3D and level plots of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import problem1 as pb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "custom_3dplot( pb1.f, pb1.x1_min,pb1.x1_max,pb1.x2_min,pb1.x2_max,pb1.nb_points, pb1.vmin, pb1.vmax )\n",
    "level_plot( pb1.f, pb1.x1_min,pb1.x1_max,pb1.x2_min,pb1.x2_max,pb1.nb_points, pb1.levels , pb1.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a. simple gradient algorithm\n",
    "\n",
    "For minimizing a differentiable function $f:\\mathbb{R}^n \\to \\mathbb{R}$, given:\n",
    "* the function to minimize `f`\n",
    "* a 1st order oracle `f_grad` (see `problem1.ipynb` for instance)\n",
    "* an initialization point `x0`\n",
    "* the sought precision `PREC` \n",
    "* a maximal number of iterations `ITE_MAX` \n",
    "\n",
    "these algorithms perform iterations of the form\n",
    "$$ x_{k+1} = x_k - \\gamma_k \\nabla f(x_k) $$\n",
    "where $\\gamma_k$ is a stepsize to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we consider the case where the stepsize is fixed over iterations and passed an argument `step` to the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the functions and variables defined in `problem1.py` <br/>\n",
    "> Observe the function `gradient_algorithm` in `algorithms.py` <br/>\n",
    "> Examine and run the cells below. Notably change the step size `step` and observe the different behaviors of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms\n",
    "from algorithms import *  # import all methods of the module into the current environment\n",
    "\n",
    "import problem1 as pb1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 20                       # Max number of iterations\n",
    "x0      = np.array( (0.0,0.0 ) )   # Initial point\n",
    "step    = 1.0/pb1.L                # Stepsize \n",
    "\n",
    "\n",
    "##### Gradient algorithm\n",
    "\n",
    "\n",
    "x,x_tab = gradient_algorithm(pb1.f , pb1.f_grad , x0 , step , PREC , ITE_MAX )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting\n",
    "\n",
    "The following cell plots the iterates over the level sets of the minimized function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "level_points_plot( pb1.f , x_tab , pb1.x1_min,pb1.x1_max,pb1.x2_min,pb1.x2_max,pb1.nb_points, pb1.levels , pb1.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.b. Newton algorithm\n",
    "\n",
    " \n",
    "For minimizing a *twice* differentiable function $f:\\mathbb{R}^n \\to \\mathbb{R}$, given:\n",
    "* the function to minimize `f`\n",
    "* a 2nd order oracle `f_grad_hessian` (see `problem1.py` for instance)\n",
    "* an initialization point `x0`\n",
    "* the sought precision `PREC` \n",
    "* a maximal number of iterations `ITE_MAX` \n",
    "\n",
    "this algorithm performs iterations of the form\n",
    " $$ x_{k+1} = x_k - [\\nabla^2 f(x_k) ]^{-1} \\nabla f(x_k) .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the functions and variables defined in `problem1.py` <br/>\n",
    "> Complete the function `newton_algorithm` as asked in `algorithms.py` <br/>\n",
    "> Examine and run the cells below. In how many iterations does the algorithm converge? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import *  # import all methods of the module into the current environment\n",
    "\n",
    "import problem1 as pb1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 20                       # Max number of iterations\n",
    "x0      = np.array( (0.0,0.0 ) )   # Initial point\n",
    "\n",
    "##### Newton algorithm\n",
    "xn,xn_tab = newton_algorithm(pb1.f , pb1.f_grad_hessian , x0  , PREC , ITE_MAX )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "level_points_plot( pb1.f , xn_tab , pb1.x1_min,pb1.x1_max,pb1.x2_min,pb1.x2_max,pb1.nb_points, pb1.levels , pb1.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Comparing gradient algorithm and Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "##### comparison\n",
    "level_2points_plot( pb1.f , x_tab , xn_tab ,  pb1.x1_min, pb1.x1_max, pb1.x2_min, pb1.x2_max, pb1.nb_points,  pb1.levels ,  pb1.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. More involved functions\n",
    "\n",
    "Now, we investigate other functions and examine the behavior of gradient algorithms in these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a  Sharper functions\n",
    "\n",
    "<a href=\"#pb3\">Problem 2</a> features a sharper function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import problem2 as pb2\n",
    "\n",
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "custom_3dplot( pb2.f, pb2.x1_min,pb2.x1_max,pb2.x2_min,pb2.x2_max,pb2.nb_points, pb2.vmin, pb2.vmax )\n",
    "level_plot( pb2.f, pb2.x1_min,pb2.x1_max,pb2.x2_min,pb2.x2_max,pb2.nb_points, pb2.levels , pb2.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fill the needed 1st and 2nd order oracles in `problem2.py`.<br/>\n",
    "> Run and compare constant stepsize gradient and Newton algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from algorithms import *  # import all methods of the module into the current environment\n",
    "import numpy as np\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 20                       # Max number of iterations\n",
    "x0      = np.array( (0.0,0.0 ) )   # Initial point\n",
    "\n",
    "##### Gradient algorithm\n",
    "step    = 1.0/pb2.L                # Stepsize \n",
    "x,x_tab = gradient_algorithm(pb2.f , pb2.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### Newton algorithm\n",
    "xn,xn_tab = newton_algorithm(pb2.f , pb2.f_grad_hessian , x0  , PREC , ITE_MAX )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Comparing gradient algorithm and Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "##### comparison\n",
    "level_points_plot( pb2.f , x_tab , pb2.x1_min,pb2.x1_max,pb2.x2_min,pb2.x2_max,pb2.nb_points, pb2.levels , pb2.title )\n",
    "#level_points_plot( pb2.f , xn_tab , pb2.x1_min,pb2.x1_max,pb2.x2_min,pb2.x2_max,pb2.nb_points, pb2.levels , pb2.title )\n",
    "#level_2points_plot( pb2.f , x_tab , xn_tab ,  pb2.x1_min, pb2.x1_max, pb2.x2_min, pb2.x2_max, pb2.nb_points,  pb2.levels ,  pb2.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Non-convex functions\n",
    "\n",
    "<a href=\"#pb3\">Problem 3</a> features classical Rosenbrock non-convex function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import problem3 as pb3\n",
    "\n",
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "custom_3dplot( pb3.f, pb3.x1_min,pb3.x1_max,pb3.x2_min,pb3.x2_max,pb3.nb_points, pb3.vmin, pb3.vmax )\n",
    "level_plot( pb3.f, pb3.x1_min,pb3.x1_max,pb3.x2_min,pb3.x2_max,pb3.nb_points, pb3.levels , pb3.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fill the needed 1st and 2nd order oracles in `problem3.py`.<br/>\n",
    "> Run and compare constant stepsize gradient and Newton algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from algorithms import *  # import all methods of the module into the current environment\n",
    "import numpy as np\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 20                       # Max number of iterations\n",
    "x0      = np.array( (0.0,0.0 ) )   # Initial point\n",
    "\n",
    "##### Gradient algorithm\n",
    "step    =  0.1                     # Stepsize \n",
    "x,x_tab = gradient_algorithm(pb3.f , pb3.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### Newton algorithm\n",
    "xn,xn_tab = newton_algorithm(pb3.f , pb3.f_grad_hessian , x0  , PREC , ITE_MAX )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Comparing gradient algorithm and Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "##### comparison\n",
    "level_points_plot( pb3.f , x_tab , pb3.x1_min,pb3.x1_max,pb3.x2_min,pb3.x2_max,pb3.nb_points, pb3.levels , pb3.title )\n",
    "#level_points_plot( pb3.f , xn_tab , pb3.x1_min,pb3.x1_max,pb3.x2_min,pb3.x2_max,pb3.nb_points, pb3.levels , pb3.title )\n",
    "#level_2points_plot( pb3.f , x_tab , xn_tab ,  pb3.x1_min, pb3.x1_max, pb3.x2_min, pb3.x2_max, pb3.nb_points,  pb3.levels ,  pb3.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> try and correct the gradient algorithm behavior by creating an adaptative stepsize algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Non-convex functions with mutliple minimizers\n",
    "\n",
    "<a href=\"#pb4\">Problem 4</a> features a smooth non-convex function with two minimizers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import problem4 as pb4\n",
    "\n",
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "custom_3dplot( pb4.f, pb4.x1_min,pb4.x1_max,pb4.x2_min,pb4.x2_max,pb4.nb_points, pb4.vmin, pb4.vmax )\n",
    "level_plot( pb4.f, pb4.x1_min,pb4.x1_max,pb4.x2_min,pb4.x2_max,pb4.nb_points, pb4.levels , pb4.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fill the needed 1st order oracles in `problem4.py`. Can you compute a second order oracle?<br/>\n",
    "> Run the constant and adaptive stepsize gradient algorithms. Change the initialization and observe the final point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from algorithms import *  # import all methods of the module into the current environment\n",
    "import numpy as np\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 100                       # Max number of iterations\n",
    "x0      = np.array( (1.5,0.55 ) )   # Initial point\n",
    "\n",
    "##### Gradient algorithm\n",
    "step    =  0.1                     # Stepsize \n",
    "x,x_tab = gradient_algorithm(pb4.f , pb4.f_grad , x0 , step , PREC , ITE_MAX )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Comparing gradient algorithm and Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "##### comparison\n",
    "level_points_plot( pb4.f , x_tab , pb4.x1_min,pb4.x1_max,pb4.x2_min,pb4.x2_max,pb4.nb_points, pb4.levels , pb4.title )\n",
    "#level_points_plot( pb4.f , xn_tab , pb4.x1_min,pb4.x1_max,pb4.x2_min,pb4.x2_max,pb4.nb_points, pb4.levels , pb4.title )\n",
    "#level_2points_plot( pb4.f , x_tab , xn_tab ,  pb4.x1_min, pb4.x1_max, pb4.x2_min, pb4.x2_max, pb4.nb_points,  pb4.levels ,  pb4.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What would you do if you would like to find *all* minimizers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d Polyhedral functions\n",
    "\n",
    "<a href=\"#pb5\">Problem 5</a> features a convex polydehral function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import problem5 as pb5\n",
    "\n",
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "custom_3dplot( pb5.f, pb5.x1_min,pb5.x1_max,pb5.x2_min,pb5.x2_max,pb5.nb_points, pb5.vmin, pb5.vmax )\n",
    "level_plot( pb5.f, pb5.x1_min,pb5.x1_max,pb5.x2_min,pb5.x2_max,pb5.nb_points, pb5.levels , pb5.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fill the needed 1st order oracles in `problem5.py`. What is different from before?<br/>\n",
    "> Run the constant and adaptive stepsize gradient algorithms. Change the initialization and observe the final point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from algorithms import *  # import all methods of the module into the current environment\n",
    "import numpy as np\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 0.01                     # Sought precision\n",
    "ITE_MAX = 100                       # Max number of iterations\n",
    "x0      = np.array( (1.5,0.55 ) )   # Initial point\n",
    "\n",
    "##### Gradient algorithm\n",
    "step    =  0.1                     # Stepsize \n",
    "x,x_tab = gradient_algorithm(pb5.f , pb5.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "xa,xa_tab = gradient_adaptive_algorithm(pb5.f , pb5.f_grad , x0 , step , PREC , ITE_MAX )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Comparing gradient algorithm and adaptive stepsize method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "##### comparison\n",
    "#level_points_plot( pb5.f , x_tab , pb5.x1_min,pb5.x1_max,pb5.x2_min,pb5.x2_max,pb5.nb_points, pb5.levels , pb5.title )\n",
    "#level_points_plot( pb5.f , xa_tab , pb5.x1_min,pb5.x1_max,pb5.x2_min,pb5.x2_max,pb5.nb_points, pb5.levels , pb5.title )\n",
    "level_2points_plot( pb5.f , x_tab , xa_tab ,  pb5.x1_min, pb5.x1_max, pb5.x2_min, pb5.x2_max, pb5.nb_points,  pb5.levels ,  pb5.title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What would you do to minimize polyhedral functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: <a id=\"pbs\">Problems</a>\n",
    "\n",
    "The problems we consider in this first lab are minimizations of unconstrained continous functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **1.** <a id=\"pb3\">`problem1`</a> features a simple quadratic function\n",
    "$$\\begin{array}{rrcll}\n",
    "f: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & 4 (x_1-3)^2 + 2(x_2-1)^2\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/1.png\" width=\"50%\"></center>\n",
    "\n",
    "\n",
    "> **2.** <a id=\"pb3\">`problem2`</a> features a more involved but very smooth function\n",
    "$$\\begin{array}{rrcll}\n",
    "g: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & \\log( 1 + \\exp(4 (x_1-3)^2 ) + \\exp( 2(x_2-1)^2 ) ) - \\log(3)\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/2.png\" width=\"50%\"></center>\n",
    "\n",
    "\n",
    "> **3.** <a id=\"pb3\">`problem3`</a> features Rosenbrock's smooth but non-convex function\n",
    "$$\\begin{array}{rrcll}\n",
    "r: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  &  (1-x_1)^2 + 100(x_2-x_1^2)^2\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/3.png\" width=\"50%\"></center>\n",
    "\n",
    "\n",
    "> **4.** <a id=\"pb4\">`problem4`</a> features a smooth function with two distinct minimizers\n",
    "$$\\begin{array}{rrcll}\n",
    "t: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & (0.6 x_1 + 0.2 x_2)^2 \\left((0.6 x_1 + 0.2 x_2)^2 - 4 (0.6 x_1 + 0.2 x_2)+4\\right) + (-0.2 x_1 + 0.6 x_2)^2\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/4.png\" width=\"50%\"></center>\n",
    "\n",
    "\n",
    "> **5.** <a id=\"pb5\">`problem5`</a> features a polyhedral function\n",
    "$$\\begin{array}{rrcll}\n",
    "p: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  &  \\left| x_1-3 \\right|  + 2\\left| x_2-1\\right| .\n",
    "\\end{array}$$\n",
    "<center><img src=\"Fig/5.png\" width=\"50%\"></center>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
