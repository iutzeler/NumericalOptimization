{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)  -- 1st year</h3></center>\n",
    "<hr>\n",
    "<center><h1>Numerical Optimization</h1></center>\n",
    "<center><h2>Lab 9: constrained optimization, basics</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from plotLib import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained optimization\n",
    "\n",
    "We consider the problem of miniizing a function $f$ under inequality constraints:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{x\\in\\mathbb R^n} f(x) \\qquad \\text{s.t.}\\qquad c_i(x)\\le 0, \\quad i=1, \\ldots, m.\n",
    "\\label{eq:gereral_problem} \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "Note that this setting also encompasses equality constraints.\n",
    "\n",
    "### Simple feasible sets\n",
    "In previous labs, we discussed the projected gradient method. It allows to tackle problems which feasible set, defined by the constraints $c_i(x)\\le 0$, can be easily projected upon. These sets were mainly $\\mathbb R^+$, $[a, b]$ where $a, b\\in\\{-\\infty\\}\\cup\\mathbb R\\cup\\{+\\infty\\}$, or products of such sets. One iteration writes\n",
    "\n",
    "\\begin{equation}\n",
    "    x_{k+1} = \\mathrm{proj}_{C} (x_k - \\gamma \\nabla f(x_k))\n",
    "\\end{equation}\n",
    "\n",
    "where $C = \\{x : c_i(x)\\le 0, \\quad i=1, \\ldots, m\\}$, and $\\gamma>0$ is a stepsize.\n",
    "\n",
    "### Generic feasible sets: primal and dual problem\n",
    "We now turn to the more general problem $\\eqref{eq:gereral_problem}$, for projection on the feasible space is a problem almost as difficult as solving the full problem. In this lab, we turn to methods stemming from the duality theory.\n",
    "\n",
    "#### Primal problem\n",
    "The so-called *primal problem* associated with $\\eqref{eq:gereral_problem}$ writes:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{x\\in\\mathbb R^n} \\max_{\\lambda \\in\\mathbb{R}_{+}^m} f(x) + \\langle \\lambda, c(x) \\rangle\n",
    "\\label{eq:primal} \\tag{$\\mathcal P$}\n",
    "\\end{equation}\n",
    "\n",
    "> **Task 1**: Compare problems $\\eqref{eq:gereral_problem}$ and $\\eqref{eq:primal}$. You may consider the function $p$ defined by:\n",
    "\n",
    "\\begin{equation}\n",
    "p(x) = \\max_{\\lambda \\in\\mathbb{R}_{+}^m} f(x) + \\langle \\lambda, c(x) \\rangle.\n",
    "\\end{equation}\n",
    "\n",
    "#### Dual problem\n",
    "One may also consider the so-called *dual problem*:\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{\\lambda \\in\\mathbb{R}_{+}^m} \\min_{x\\in\\mathbb R^n} f(x) + \\langle \\lambda, c(x) \\rangle,\n",
    "\\label{eq:dual} \\tag{$\\mathcal D$}\n",
    "\\end{equation}\n",
    "\n",
    "and define the *dual function*\n",
    "\n",
    "\\begin{equation}\n",
    "q(\\lambda) = \\min_{x\\in\\mathbb R^n} f(x) + \\langle \\lambda, c(x) \\rangle.\n",
    "\\end{equation}\n",
    "\n",
    "You may recognize here the basic objects on which the duality theory builds, including the Langrangian function of $\\eqref{eq:gereral_problem}$:\n",
    "\n",
    "$$L(x, \\lambda) = f(x) + \\langle \\lambda, c(x)\\rangle .$$\n",
    "\n",
    "Under suitable assumptions, all solutions $(\\bar{x}, \\bar{\\lambda})$ of problem $\\eqref{eq:primal}$ are solutions of problem $\\eqref{eq:dual}$, and in particular, $\\bar{x}$ is a solution of problem $\\eqref{eq:gereral_problem}$. The problems considerd in this lab all fall into this well-behaved category.\n",
    "\n",
    "#### Solving the dual\n",
    "The projected gradient attempts to solve $\\eqref{eq:primal}$ directly, which makes it a *primal method*. We consider *dual methods*, designed to solve $\\eqref{eq:dual}$. In doing so, we trade the problem of minimizing an explicit function on a difficult set by that of minimizing a complex function (defined as a minimization) on a simple set.\n",
    "\n",
    "_Fact_: Let $\\lambda\\in\\mathbb R^m$ such that a solution to $\\min_{x\\in\\mathbb R^n} f(x) + \\langle \\lambda, c(x) \\rangle$ exists, which we denote $\\bar{x}_\\lambda$. If $\\bar{x}_\\lambda$ is the unique solution of this problem, then\n",
    "\n",
    "\\begin{cases} \n",
    "q(\\lambda ) = f(\\bar{x}_\\lambda) + \\langle \\lambda, c(\\bar{x}_\\lambda) \\rangle \\\\\n",
    "\\nabla q(\\lambda ) = c(\\bar{x}_\\lambda)\n",
    "\\end{cases}\n",
    "\n",
    "> **Task 2**: What algorithm could be employed to solve $\\eqref{eq:dual}$?\n",
    "\n",
    "The goal of this lab is to implement this algorithm. We'll require a stopping criterion for that algorithm, and example problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple problem\n",
    "\n",
    "As a first problem, we consider:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{x}{\\text{minimize}}\n",
    "& & 4 (x_1-3)^2 + 2 (x_2-1)^2 \\\\\n",
    "& \\text{subject to}\n",
    "& & x_1 - x_2 - 1 \\le 0\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "> **Task 3**: implement the oracles in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePb:\n",
    "    def f(self, x):\n",
    "        return 4*(x[0]-3)**2+2*(x[1]-1)**2\n",
    "\n",
    "    def f_grad(self, x):\n",
    "        # TODO\n",
    "        return np.array([0, 0])\n",
    "\n",
    "    def f_grad_hessian(self, x):\n",
    "        # TODO\n",
    "        g = np.array([0, 0])\n",
    "        H = np.array([(0, 0), (0, 0)])\n",
    "        return g, H\n",
    "\n",
    "    def c1(self, x):\n",
    "        return x[0] - x[1] - 1\n",
    "\n",
    "    def c(self, x):\n",
    "        return np.array([ c1(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplepb = SimplePb()\n",
    "\n",
    "x1_min = -0.5\n",
    "x1_max = 5.5\n",
    "x2_min = -0.5\n",
    "x2_max = 2.5\n",
    "levels = [0.5,1,2,5,10,15]\n",
    "level_plot(simplepb.f, x1_min, x1_max, x2_min, x2_max, 200, levels , 'f: quadratic' )\n",
    "\n",
    "c1levels = [-3,-2,-1,0,1,2,3]\n",
    "level_plot( simplepb.c1, x1_min, x1_max, x2_min, x2_max, 200, c1levels , 'c_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 4**: Given the graphs, what is the minimizer pair $(\\bar{x}, \\bar{\\lambda})$ for this problem.\n",
    "\n",
    "## A stopping criterion\n",
    "The algorithm coming up will generate a sequence of iterates that should get close to a solution of the initial constrained problem. We seek to formulate a criterion which determines if a given point is a solution, up to some precision. To do so, we first need to formalise the notion of solution of the constrained problem, and determine what are the mathematical properties of such points.\n",
    "\n",
    "> **Task 5**: \n",
    "> 1. How does one formalizes the notion of \"solution\" of the constrained problem?\n",
    "> 2. What properties do these points all verify? In other words, how does the necessary conditions of optimality write?\n",
    "> 3. What parts of these relations need to be relaxed when implementing numerically?\n",
    "\n",
    "## The gradient ascent algorithm\n",
    "The *gradient ascent* algorithm, also known as *Uzawa's algorithm* consists in the following iteration:\n",
    "\n",
    "\\begin{equation*}\\left|\n",
    "\\begin{array}{l}\n",
    "    x_{k+1} = \\arg\\min_{x\\in\\mathbb R^n} f(x) + \\langle \\lambda^k, c(x)\\rangle \\\\ \n",
    "    \\lambda_{k+1} = \\mathrm{proj}_{\\mathbb{R}_-^m}\\left( \\lambda^k + \\gamma c(x_{k+1}) \\right)\n",
    "\\end{array}\\right.\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\gamma>0$ is some chosen stepsize.\n",
    "\n",
    "> **Task 6**: The frist part of the iteration consists in solving an otpimization problem. What kind of problem is it? What ways to solve it can you think of? Which is most relevant?\n",
    "\n",
    "*Note*: \n",
    "- You may find it useful for this task to peek at the problems considered next. Overall, there will be several constraints ($m>1$), but all individual constraints will be quadratic functions. How does one solve a quadratic optimization problem?\n",
    "- depending on how you choose to solve this subproblem, you may want to add functions computing the lagrangian value, gradient and hessian to the classes defining the problems.\n",
    "\n",
    "> **Task 7**: Implement the Uzawa algorithm as described above, and check it on the above toy problem.\n",
    "\n",
    "*Note*: gradient ascent converges for any starting point, and for stepsizes $\\gamma \\in (0, 2\\mu/\\tau^2)$, where $\\mu$ is a strong convexity constant for $f$ and $\\tau$ a Lipschitz continuity constant for $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More involved problems\n",
    "\n",
    "You may now experiment with your algorithm on more involved problems.\n",
    "\n",
    "### 1. Two affine active constraints\n",
    "\n",
    "We consider the problem\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{x}{\\text{minimize}}\n",
    "& & 4 (x_1-4)^2 + 2 (x_2-1)^2 \\\\\n",
    "& \\text{subject to}\n",
    "& & 2 x_1 - x_2 - 4 \\le 0 \\\\\n",
    "& & & x_1 - 3 \\le 0\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "> **Task 8**: implement the oracles in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveAffinePb:\n",
    "    def f(self, x):\n",
    "        return 4*(x[0]-4)**2+2*(x[1]-1)**2\n",
    "\n",
    "    def f_grad(self, x):\n",
    "        # TODO\n",
    "        return np.array([0, 0])\n",
    "\n",
    "    def f_grad_hessian(self, x):\n",
    "        # TODO\n",
    "        g = np.array([0, 0])\n",
    "        H = np.array([(0, 0), (0, 0)])\n",
    "        return g, H\n",
    "\n",
    "    def c1(self, x):\n",
    "        return 2*x[0] - x[1] - 4\n",
    "    \n",
    "    def c2(self, x):\n",
    "        return x[0] - 3\n",
    "\n",
    "    def c(self, x):\n",
    "        return np.array([ c1(x), c2(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activeaffpb = ActiveAffinePb()\n",
    "\n",
    "x1_min = -0.5\n",
    "x1_max = 5.5\n",
    "x2_min = -0.5\n",
    "x2_max = 2.5\n",
    "levels = [0.5,1,2,5,10,15]\n",
    "level_plot(activeaffpb.f, x1_min, x1_max, x2_min, x2_max, 200, levels , 'f: quadratic' )\n",
    "\n",
    "c1levels = [-3,-2,-1,0]\n",
    "level_plot( activeaffpb.c1, x1_min, x1_max, x2_min, x2_max, 200, c1levels , 'c_1')\n",
    "c2levels = [-3,-2,-1,0]\n",
    "level_plot( activeaffpb.c2, x1_min, x1_max, x2_min, x2_max, 200, c2levels , 'c_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 9**: Run your algorithm on the above problem. Inspect the solution pair, and propose a geometrical interpretation of the KKT conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. An ellipse constraint\n",
    "\n",
    "We consider the problem\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{x}{\\text{minimize}}\n",
    "& & 4 (x_1-4)^2 + 2 (x_2-1)^2 \\\\\n",
    "& \\text{subject to}\n",
    "& & x_1 - x_2 - 1 \\le 0\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "> **Task 8**: implement the oracles in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EllipseCstrPb:\n",
    "    def f(self, x):\n",
    "        return 4*(x[0]-4)**2+2*(x[1]-1)**2\n",
    "\n",
    "    def f_grad(self, x):\n",
    "        # TODO\n",
    "        return np.array([0, 0])\n",
    "\n",
    "    def f_grad_hessian(self, x):\n",
    "        # TODO\n",
    "        g = np.array([0, 0])\n",
    "        H = np.array([(0, 0), (0, 0)])\n",
    "        return g, H\n",
    "\n",
    "    def c1(self, x):\n",
    "        return 0.5 * (x[0] - 1)**2 + x[1]**2 - 1\n",
    "\n",
    "    def c(self, x):\n",
    "        return np.array([ c1(x) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipsecstrpb = EllipseCstrPb()\n",
    "\n",
    "x1_min = -0.5\n",
    "x1_max = 5.5\n",
    "x2_min = -0.5\n",
    "x2_max = 2.5\n",
    "levels = [0.5,1,2,5,10,15]\n",
    "level_plot(ellipsecstrpb.f, x1_min, x1_max, x2_min, x2_max, 200, levels , 'f: quadratic' )\n",
    "\n",
    "c1levels = [-0.5, 0, 1, 2, 4, 8]\n",
    "level_plot( ellipsecstrpb.c1, x1_min, x1_max, x2_min, x2_max, 200, c1levels , 'c_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "The above method is known as *gradient ascent*, or the *Uzawa method*. It is one of the most elementary *dual methods* for solving constrained optimization programs. As such, it has several drawbacks:\n",
    "- each iteration requires solving one optimization problem;\n",
    "- its convergence garantees are quite restrictive (notably, $f$ should be strongly convex).\n",
    "\n",
    "The first issue can be dealt with by doing only a partial optimization of the subproblem. Performing one gradient step yields the *Arrow-Hurwicz* algorithm:\n",
    "\n",
    "\\begin{equation*}\\left|\n",
    "\\begin{array}{l}\n",
    "    x_{k+1} = x_k - \\epsilon \\nabla_x L(x_k, \\lambda_k) \\\\ \n",
    "    \\lambda_{k+1} = \\mathrm{proj}_{\\mathbb{R}_-^m}\\left( \\lambda^k + \\gamma \\nabla_\\lambda L(x_{k+1}, \\lambda_k) \\right)\n",
    "\\end{array}\\right.\n",
    "\\end{equation*}\n",
    "for some stepsizes $\\epsilon, \\gamma>0$, and $\\nabla_x L(x, \\lambda)$ denotes the gradient of $L(\\cdot, \\lambda)$ (thus relative to the $x$ vairable).\n",
    "\n",
    "The second is more difficult to alleviate. One prolific research direction consisted in changing the structure of the langrangian (and thus the notion of duality used). One very popular algorithm of this last decade is the *Alternating Direction Method of Multiplier*, or *ADMM*.\n",
    "\n",
    "Finally, as for unconstrained optimization, there exist methods which converge locally fast (*quadratically*) for constrained problems. The *Sequential Quadratic Programming* (*SQP*) class of algorithms is a typical with fast local convergence. Besides, one should keep in mind that for small to medium sized problems, *interior point methods* are the best methods in term of speed of convergence.\n",
    "\n",
    "#### References:\n",
    "- gradient, newton and interior point methods: _Numerical Optimization_, J. Frédéric Bonnans, J. Charles Gilbert, Claude Lemaréchal, and Claudia Sagastizabal;\n",
    "- more on ADMM and dual methods: _Distributed Optimization and StatisticalLearning via the Alternating Direction Method of Multipliers_, S. Boyd _et al_;\n",
    "- SQP and a nice reference for most methods seen in the labs: _Numerical Optimization_, J. Nocedal, S. Wright;"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
