{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression Problem\n",
    "\n",
    "\n",
    "\n",
    "### Machine Learning as an Optimization problem\n",
    "\n",
    "We have some *data*  $\\mathcal{D}$ consisting of $m$ *examples* $\\{d_i\\}$; each example consisting of a *feature* vector $a_i\\in\\mathbb{R}^d$ and an *observation* $b_i\\in \\mathcal{O}$: $\\mathcal{D} = \\{[a_i,b_i]\\}_{i=1..m}$. In this lab, we will consider the <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.names\">ionosphere</a> dataset.\n",
    "\n",
    "\n",
    "The goal of *supervised learning* is to construct a predictor for the observations when given feature vectors.\n",
    "\n",
    "\n",
    "A popular approach is based on *linear models* which are based on finding a *parameter* $x$ such that the real number $\\langle a_i , x \\rangle$ is used to predict the value of the observation through a *predictor function* $g:\\mathbb{R}\\to \\mathcal{O}$: $g(\\langle a_i , x \\rangle)$ is the predicted value from $a_i$.\n",
    "\n",
    "\n",
    "In order to find such a parameter, we use the available data and a *loss* $\\ell$ that penalizes the error made between the predicted $g(\\langle a_i , x \\rangle)$ and observed $b_i$ values. For each example $i$, the corresponding error function for a parameter $x$ is $f_i(x) =   \\ell( g(\\langle a_i , x \\rangle) ; b_i )$. Using the whole data, the parameter that minimizes the total error is the solution of the minimization problem\n",
    "$$ \\min_{x\\in\\mathbb{R}^d}  \\frac{1}{m} \\sum_{i=1}^m f_i(x) = \\frac{1}{m} \\sum_{i=1}^m  \\ell( g(\\langle a_i , x \\rangle) ; b_i ). $$\n",
    "\n",
    "\n",
    "### Binary Classification with Logisitic Regression\n",
    "\n",
    "In our setup, the observations are binary: $\\mathcal{O} = \\{-1 , +1 \\}$, and the *Logistic loss* is used to form the following optimization problem\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\frac{\\lambda}{2} \\|x\\|_2^2.\n",
    "\\end{align*}\n",
    "where the last term is added as a regularization (of type $\\ell_2$, aka Tikhnov) to prevent overfitting.\n",
    "\n",
    "Under some statistical hypotheses, $x^\\star = \\arg\\min f(x)$ maximizes the likelihood of the labels knowing the features vector. Then, for a new point $d$ with features vector $a$, \n",
    "$$ p_1(a) = \\mathbb{P}[d\\in \\text{ class }  +1] = \\frac{1}{1+\\exp(-\\langle a;x^\\star \\rangle)} $$\n",
    "Thus, from $a$, if $p_1(a)$ is close to $1$, one can decide that $d$ belongs to class $1$; and the opposite decision if $p(a)$ is close to $0$. Between the two, the appreciation is left to the data scientist depending on the application.\n",
    "\n",
    "\n",
    "# Objective of the optimizer\n",
    "\n",
    "Given oracles for the function and its gradient, as well as an upper-bound of the Lipschitz constant $L$ of the gradient, find a minimizer of $f$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "\n",
    "file = open('ionosphere.data')\n",
    "\n",
    "d = 34\n",
    "n = d+1 # Variable size + intercept\n",
    "\n",
    "m = 351 # Number of examples\n",
    "\n",
    "lam = 0.001 # regularization best:0.001\n",
    "\n",
    "A = np.zeros((m,d))\n",
    "b = np.zeros(m)\n",
    "\n",
    "reader = csv.reader(file, delimiter=',')\n",
    "i = 0\n",
    "for row in reader:\n",
    "    A[i] = np.array(row[:d]) \n",
    "    if row[d] == 'b':\n",
    "        b[i] = -1.0\n",
    "    else:\n",
    "        b[i] =  1.0 \n",
    "    i+=1\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(A)\n",
    "A = scaler.transform(A)  \n",
    "\n",
    "# Adding an intercept\n",
    "A_inter = np.ones((m,n))\n",
    "A_inter[:,:-1] = A\n",
    "A = A_inter\n",
    "\n",
    "\n",
    "L = 0.25*max(np.linalg.norm(A,2,axis=1))**2 + lam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    l = 0.0\n",
    "    for i in range(A.shape[0]):\n",
    "        if b[i] > 0 :\n",
    "            l += np.log( 1 + np.exp(-np.dot( A[i] , x ) ) ) \n",
    "        else:\n",
    "            l += np.log( 1 + np.exp(np.dot( A[i] , x ) ) ) \n",
    "    return l/m + lam/2.0*np.dot(x,x)\n",
    "\n",
    "def f_grad(x):\n",
    "    g = np.zeros(n)\n",
    "    for i in range(A.shape[0]):\n",
    "        if b[i] > 0:\n",
    "            g += -A[i]/( 1 + np.exp(np.dot( A[i] , x ) ) ) \n",
    "        else:\n",
    "            g += A[i]/( 1 + np.exp(-np.dot( A[i] , x ) ) ) \n",
    "    return g/m + lam*x\n",
    "\n",
    "def f_grad_hessian(x):\n",
    "    g = np.zeros(n)\n",
    "    H = np.zeros((n,n))\n",
    "    for i in range(A.shape[0]):\n",
    "        if b[i] > 0:\n",
    "            g += -A[i]/( 1 + np.exp(np.dot( A[i] , x ) ) ) \n",
    "            H +=  (np.exp(np.dot( A[i] , x ))/( 1 + np.exp(np.dot( A[i] , x ) ) )**2)*np.outer(A[i],A[i])\n",
    "        else:\n",
    "            g += A[i]/( 1 + np.exp(-np.dot( A[i] , x ) ) ) \n",
    "            H +=  (np.exp(-np.dot( A[i] , x ))/( 1 + np.exp(-np.dot( A[i] , x ) ) )**2)*np.outer(A[i],A[i])\n",
    "    g =  g/m + lam*x\n",
    "    H = H/m + lam*np.eye(n)\n",
    "    return g,H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(w,PRINT):\n",
    "    pred = np.zeros(A.shape[0])\n",
    "    perf = 0\n",
    "    for i in range(A.shape[0]):\n",
    "        p = 1.0/( 1 + np.exp(-np.dot( A[i] , w ) ) )\n",
    "        if p>0.5:\n",
    "            pred[i] = 1.0\n",
    "            if b[i]>0:\n",
    "                correct = \"True\"\n",
    "                perf += 1\n",
    "            else:\n",
    "                correct = \"False\"\n",
    "            if PRINT:\n",
    "                print(\"True class: {:d} \\t-- Predicted: {} \\t(confidence: {:.1f}%)\\t{}\".format(int(b[i]),1,(p-0.5)*200,correct))\n",
    "        else:\n",
    "            pred[i] = -1.0\n",
    "            if b[i]<0:\n",
    "                correct = \"True\"\n",
    "                perf += 1\n",
    "            else:\n",
    "                correct = \"False\"\n",
    "            if PRINT:\n",
    "                print(\"True class: {:d} \\t-- Predicted: {} \\t(confidence: {:.1f}%)\\t{}\".format(int(b[i]),-1,100-(0.5-p)*200,correct))\n",
    "    return pred,float(perf)/A.shape[0]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
