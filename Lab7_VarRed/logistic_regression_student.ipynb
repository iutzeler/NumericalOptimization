{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Problem\n",
    "\n",
    "\n",
    "\n",
    "### Machine Learning as an Optimization problem\n",
    "\n",
    "We have some *data*  $\\mathcal{D}$ consisting of $m$ *examples* $\\{d_i\\}$; each example consisting of a *feature* vector $a_i\\in\\mathbb{R}^d$ and an *observation* $b_i\\in \\mathcal{O}$: $\\mathcal{D} = \\{[a_i,b_i]\\}_{i=1..m}$. In this lab, we will consider the <a href=\"http://archive.ics.uci.edu/ml/datasets/Student+Performance\">student performance</a> dataset.\n",
    "\n",
    "\n",
    "The goal of *supervised learning* is to construct a predictor for the observations when given feature vectors.\n",
    "\n",
    "\n",
    "A popular approach is based on *linear models* which are based on finding a *parameter* $x$ such that the real number $\\langle a_i , x \\rangle$ is used to predict the value of the observation through a *predictor function* $g:\\mathbb{R}\\to \\mathcal{O}$: $g(\\langle a_i , x \\rangle)$ is the predicted value from $a_i$.\n",
    "\n",
    "\n",
    "In order to find such a parameter, we use the available data and a *loss* $\\ell$ that penalizes the error made between the predicted $g(\\langle a_i , x \\rangle)$ and observed $b_i$ values. For each example $i$, the corresponding error function for a parameter $x$ is $f_i(x) =   \\ell( g(\\langle a_i , x \\rangle) ; b_i )$. Using the whole data, the parameter that minimizes the total error is the solution of the minimization problem\n",
    "$$ \\min_{x\\in\\mathbb{R}^d}  \\frac{1}{m} \\sum_{i=1}^m f_i(x) = \\frac{1}{m} \\sum_{i=1}^m  \\ell( g(\\langle a_i , x \\rangle) ; b_i ). $$\n",
    "\n",
    "\n",
    "\n",
    "# Regularized Problem \n",
    "\n",
    "In this lab, we will consider an $\\ell_1$ regularization to promote sparsity of the iterates. A sparse final solution would select the most important features. The new function (below) is non-smooth but it has a smooth part, $f$, the same as in Lab3; and a non-smooth part, $g$, that we will treat with proximal operations.\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } F(x) := \\underbrace{ \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\frac{\\lambda_2}{2} \\|x\\|_2^2}_{f(x)} + \\underbrace{\\lambda_1 \\|x\\|_1 }_{g(x)}.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "# Features signification [NEW]\n",
    "\n",
    "The dataset is comprised of $27$ features described below and the goal is to predict if the student may pass its year or not. It is thus of importance to investigate which features are the most significant for the student success. We will see how the $\\ell_1$ regularization can help to this goal."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 sex - student's sex (binary: \"F\" - female or \"M\" - male)\n",
    "2 age - student's age (numeric: from 15 to 22)\n",
    "3 address - student's home address type (binary: \"U\" - urban or \"R\" - rural)\n",
    "4 famsize - family size (binary: \"LE3\" - less or equal to 3 or \"GT3\" - greater than 3)\n",
    "5 Pstatus - parent's cohabitation status (binary: \"T\" - living together or \"A\" - apart)\n",
    "6 Medu - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
    "7 Fedu - father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
    "8 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
    "9 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
    "10 failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n",
    "11 schoolsup - extra educational support (binary: yes or no)\n",
    "12 famsup - family educational support (binary: yes or no)\n",
    "13 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "14 activities - extra-curricular activities (binary: yes or no)\n",
    "15 nursery - attended nursery school (binary: yes or no)\n",
    "16 higher - wants to take higher education (binary: yes or no)\n",
    "17 internet - Internet access at home (binary: yes or no)\n",
    "18 romantic - with a romantic relationship (binary: yes or no)\n",
    "19 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
    "20 freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n",
    "21 goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n",
    "22 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "23 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "24 health - current health status (numeric: from 1 - very bad to 5 - very good)\n",
    "25 absences - number of school absences (numeric: from 0 to 93)\n",
    "26 G1 - first period grade (numeric: from 0 to 20)\n",
    "27 G2 - second period grade (numeric: from 0 to 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#### File reading\n",
    "dat_file = np.load('student.npz')\n",
    "A = dat_file['A_learn']\n",
    "final_grades = dat_file['b_learn']\n",
    "m = final_grades.size\n",
    "b = np.zeros(m)\n",
    "for i in range(m):\n",
    "    if final_grades[i]>11:\n",
    "        b[i] = 1.0\n",
    "    else:\n",
    "        b[i] = -1.0\n",
    "\n",
    "A_test = dat_file['A_test']\n",
    "final_grades_test = dat_file['b_test']\n",
    "m_test = final_grades_test.size\n",
    "b_test = np.zeros(m_test)\n",
    "for i in range(m_test):\n",
    "    if final_grades_test[i]>11:\n",
    "        b_test[i] = 1.0\n",
    "    else:\n",
    "        b_test[i] = -1.0\n",
    "\n",
    "\n",
    "d = 27 # features\n",
    "n = d+1 # with the intercept\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lam2 = 0.1 # for the 2-norm regularization best:0.1\n",
    "lam1 = 0.03 # for the 1-norm regularization best:0.03\n",
    "\n",
    "\n",
    "L = 0.25*max(np.linalg.norm(A,2,axis=1))**2 + lam2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracles\n",
    "\n",
    "### Related to function $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    l = 0.0\n",
    "    for i in range(A.shape[0]):\n",
    "        if b[i] > 0 :\n",
    "            l += np.log( 1 + np.exp(-np.dot( A[i] , x ) ) ) \n",
    "        else:\n",
    "            l += np.log( 1 + np.exp(np.dot( A[i] , x ) ) ) \n",
    "    return l/m + lam2/2.0*np.dot(x,x)\n",
    "\n",
    "def f_grad(x):\n",
    "    g = np.zeros(n)\n",
    "    for i in range(A.shape[0]):\n",
    "        if b[i] > 0:\n",
    "            g += -A[i]/( 1 + np.exp(np.dot( A[i] , x ) ) ) \n",
    "        else:\n",
    "            g += A[i]/( 1 + np.exp(-np.dot( A[i] , x ) ) ) \n",
    "    return g/m + lam2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related to function $f_i$ (one example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_grad_ex(x,i):\n",
    "    g = np.zeros(n)\n",
    "    \n",
    "    #### TODO\n",
    "    \n",
    "    return g/m + lam2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related to function $g$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return lam1*np.linalg.norm(x,1)\n",
    "\n",
    "def g_prox(x,gamma):\n",
    "    p = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        if x[i] < - lam1*gamma:\n",
    "            p[i] = x[i] + lam1*gamma\n",
    "        if x[i] > lam1*gamma:\n",
    "            p[i] = x[i] - lam1*gamma\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related to function $F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x):\n",
    "    return f(x) + g(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_train(w,PRINT):\n",
    "    pred = np.zeros(A.shape[0])\n",
    "    perf = 0\n",
    "    for i in range(A.shape[0]):\n",
    "        p = 1.0/( 1 + np.exp(-np.dot( A[i] , w ) ) )\n",
    "        if p>0.5:\n",
    "            pred[i] = 1.0\n",
    "            if b[i]>0:\n",
    "                correct = \"True\"\n",
    "                perf += 1\n",
    "            else:\n",
    "                correct = \"False\"\n",
    "            if PRINT:\n",
    "                print(\"True class: {:d} \\t-- Predicted: {} \\t(confidence: {:.1f}%)\\t{}\".format(int(b[i]),1,(p-0.5)*200,correct))\n",
    "        else:\n",
    "            pred[i] = -1.0\n",
    "            if b[i]<0:\n",
    "                correct = \"True\"\n",
    "                perf += 1\n",
    "            else:\n",
    "                correct = \"False\"\n",
    "            if PRINT:\n",
    "                print(\"True class: {:d} \\t-- Predicted: {} \\t(confidence: {:.1f}%)\\t{}\".format(int(b[i]),-1,100-(0.5-p)*200,correct))\n",
    "    return pred,float(perf)/A.shape[0]\n",
    "\n",
    "def prediction_test(w,PRINT):\n",
    "    pred = np.zeros(A_test.shape[0])\n",
    "    perf = 0\n",
    "    for i in range(A_test.shape[0]):\n",
    "        p = 1.0/( 1 + np.exp(-np.dot( A_test[i] , w ) ) )\n",
    "        if p>0.5:\n",
    "            pred[i] = 1.0\n",
    "            if b_test[i]>0:\n",
    "                correct = \"True\"\n",
    "                perf += 1\n",
    "            else:\n",
    "                correct = \"False\"\n",
    "            if PRINT:\n",
    "                print(\"True class: {:d} \\t-- Predicted: {} \\t(confidence: {:.1f}%)\\t{}\".format(int(b[i]),1,(p-0.5)*200,correct))\n",
    "        else:\n",
    "            pred[i] = -1.0\n",
    "            if b_test[i]<0:\n",
    "                correct = \"True\"\n",
    "                perf += 1\n",
    "            else:\n",
    "                correct = \"False\"\n",
    "            if PRINT:\n",
    "                print(\"True class: {:d} \\t-- Predicted: {} \\t(confidence: {:.1f}%)\\t{}\".format(int(b[i]),-1,100-(0.5-p)*200,correct))\n",
    "    return pred,float(perf)/A_test.shape[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
