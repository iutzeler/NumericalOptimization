{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)  - 1st year</h3></center>\n",
    "<hr>\n",
    "<center><h1>Numerical Optimization</h1></center>\n",
    "<center><h2>Lab 7: Variance-Reduced Stochastic Gradient</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Problem\n",
    " \n",
    " \n",
    " \n",
    "### Machine Learning as an Optimization problem\n",
    " \n",
    "We have some *data*  $\\mathcal{D}$ consisting of $m$ *examples* $\\{d_i\\}$; each example consisting of a *feature* vector $a_i\\in\\mathbb{R}^d$ and an *observation* $b_i\\in \\mathcal{O}$: $\\mathcal{D} = \\{[a_i,b_i]\\}_{i=1..m}$. In this lab, we will consider the <a href=\"http://archive.ics.uci.edu/ml/datasets/Student+Performance\">student performance</a> dataset.\n",
    " \n",
    " \n",
    "The goal of *supervised learning* is to construct a predictor for the observations when given feature vectors.\n",
    " \n",
    " \n",
    " A popular approach is based on *linear models* which are based on finding a *parameter* $x$ such that the real number $\\langle a_i , x \\rangle$ is used to predict the value of the observation through a *predictor function* $g:\\mathbb{R}\\to \\mathcal{O}$: $g(\\langle a_i , x \\rangle)$ is the predicted value from $a_i$.\n",
    " \n",
    " \n",
    " In order to find such a parameter, we use the available data and a *loss* $\\ell$ that penalizes the error made between the predicted $g(\\langle a_i , x \\rangle)$ and observed $b_i$ values. For each example $i$, the corresponding error function for a parameter $x$ is $f_i(x) =   \\ell( g(\\langle a_i , x \\rangle) ; b_i )$. Using the whole data, the parameter that minimizes the total error is the solution of the minimization problem\n",
    " $$ \\min_{x\\in\\mathbb{R}^d}  \\frac{1}{m} \\sum_{i=1}^m f_i(x) = \\frac{1}{m} \\sum_{i=1}^m  \\ell( g(\\langle a_i , x \\rangle) ; b_i ). $$\n",
    " \n",
    " \n",
    " \n",
    "### Regularized Problem \n",
    " \n",
    "In this lab, we will consider an $\\ell_1$ regularization to promote sparsity of the iterates. A sparse final solution would select the most important features. The new function (below) is non-smooth but it has a smooth part, $f$; and a non-smooth part, $g$, that we will treat with proximal operations.\n",
    " \n",
    " \\begin{align*}\n",
    "     \\min_{x\\in\\mathbb{R}^d } F(x) := \\underbrace{\\frac{1}{m}  \\sum_{i=1}^m \\overbrace{  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\frac{\\lambda_2}{2} \\|x\\|_2^2 }^{f_i(x)} }_{f(x)} + \\underbrace{\\lambda_1 \\|x\\|_1 }_{g(x)}.\n",
    " \\end{align*}\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Recall of the proximal gradient algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algoProx import *      # import all methods of the module into the current environment\n",
    "import numpy as np\n",
    "import logistic_regression_student as pb\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 1000                      # Max number of iterations\n",
    "x0      = np.zeros(pb.n)              # Initial point\n",
    "step    = 1.0/pb.L\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = proximal_gradient_algorithm(pb.F , pb.f_grad , pb.g_prox , x0 , step , PREC, ITE_MAX , True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decrease of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "F = []\n",
    "for i in range(x_tab.shape[0]):\n",
    "    F.append( pb.F(x_tab[i])) \n",
    "\n",
    "plt.figure()\n",
    "plt.plot( F, color=\"black\", linewidth=1.0, linestyle=\"-\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support of the vector $x_k$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for i in np.arange(0,x_tab.shape[0],int(x_tab.shape[0]/40)):\n",
    "    for j in range(pb.n):\n",
    "        if np.abs(x_tab[i,j])>1e-14:\n",
    "            plt.plot( i , j  , 'ko')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylabel('Non-null Coordinates')\n",
    "plt.xlabel('Nb. Iterations')\n",
    "plt.ylim(-1,pb.d+1)\n",
    "plt.yticks(np.arange(0,pb.d+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Stochastic gradient \n",
    "\n",
    "\n",
    "In the following, instead of considering $f$ as a whole, we will use its structure \n",
    "$$ f(x) := \\frac{1}{m}\\sum_{i=1}^m f_i(x)$$\n",
    "\n",
    "> Implement the gradient related to $f_i$, related to one example, in `logistic_regression_student.py`\n",
    "\n",
    "With this structure a popular minimization algorithm is the *stochastic gradient algorithm* which writes as follows:\n",
    "* Select uniformly $i$ in $1,..,m$\n",
    "* $x_{k+1} = \\mathbf{prox}_{\\gamma_k g}\\left( x_k - \\gamma_k \\nabla f_i(x_k) \\right) $\n",
    "\n",
    "> Implement this algorithm with a stepsize vanishing as $1/k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance reduction\n",
    "\n",
    "The poor performance of this algorithm is notably due to the variance of the gradients. In order to overcome it, *variance reduced* algorithms have been proposed.\n",
    "\n",
    "We will consider here the popular **SAGA** algorithm  (SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives\n",
    "A Defazio, F Bach, S Lacoste-Julien, NIPS 2014. ) \n",
    "\n",
    "> Implement SAGA from the paper ( http://papers.nips.cc/paper/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives ) and compare with the stochastic gradient algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
